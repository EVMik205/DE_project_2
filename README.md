# Анализ рынка валют
Проект по построению ETL-процесса формирования витрин данных для анализа изменений курса акций.

## Описание задачи
Данные о курсах акций (5-10 компаний на выбор) должны загружаться по API Alpha Vantage (https://www.alphavantage.co/).
Затем необходимо построить витрину данных, которая содержит поля:
* cуррогатный ключ категории;
* название компании (биржевой тикер);
* суммарный объем торгов за последние сутки;
* курс на момент открытия торгов для данных суток;
* курс на момент закрытия торгов для данных суток;
* разница(в %) курса с момента открытия до момента закрытия торгов для данных суток;
* минимальный временной интервал, на котором был зафиксирован самый крупный объем торгов для данных суток;
* минимальный временной интервал, на котором был зафиксирован максимальный курс для данных суток;
* минимальный временной интервал, на котором был зафиксирован минимальный курс торгов для данных суток.

Загрузка данных должна происходить в двух режимах:
* инициализирующий – загрузка полного слепка данных источника;
* инкрементальный – загрузка дельты данных за прошедшие сутки.

Структура хранения данных должна содержать:
* сырой слой данных;
* промежуточный слой;
* слой витрин.

## Используемый стек
Так как итоговая витрина одна, сроки сжатые, то было принято решение для 
построения витрины данных не использовать Spark, а ограничиться SQL. Данных относительно 
немного, большая скорость доступа и параллелизм не требуется, следовательно для хранения 
бэкапа сырых данных будет достаточно обычной файловой системы и csv файлов.

В качестве оркестратора остановился на Airflow из-за удобства просмотра логов через
веб-интерфейс и возможности выстраивать логику работы задач с сенсорами и условиями 
(в дальнейшем проект предполагается дорабатывать).

Хранилище данных - Postgres, добавленный в докер контейнер Airflow.

## Блок-схема
```
              xxx      xx
           xxxx  xxxxx   xxx
          x                 xxx                                             +---------------------------+
      xxxxx                 x  xxx                +----------------+        |                           |
    xx                            x               |                |        |   PostgresSQL DB          |
    x     API Alpha Vantage       xxxx    ------> | Airflow DAGs   | <--->  |                           |
 xxxxxx                               x           | 127.0.0.1:8080 |        | host.docker.internal:5430 |
xx    https://www.alphavantage.co/    x           +----------------+        |                           |
x                               x     x                                     +---------------------------+
xx      xx         x            xxxxxx                    |
  xxxxxxxxx       xx          xxx                         v
           x xxx x xxxxxxxxxx                      +---------------+
                                                   |               |
                                                   |  CSV files    |
                                                   |  data/*.csv   |
                                                   +---------------+
```

## Инициализация проекта
Для первоначального запуска докер контейнеров, инициализации Airflow используется скрипт
bootstrap.sh, который выполняет:
* задаёт параметры Postgres, API, список тикеров компаний (переменные установлены в начале, при необходимости
  могут быть изменены);
* создаёт папку data для хранения csv файлов и даёт права на запись в неё;
* построение и запуск докер-контейнеров (docker/docker-compose.yaml);
* ждёт пока поднимется Airflow;
* создаёт переменные Airflow;
* копирует скрипты DAG'ов в папку docker/dags.
Затем необходимо открыть веб-интерфейс Airflow (127.0.0.1:8080 airflow/airflow) и подождать несколько 
минут. Когда появятся DAGи - включить их.

## Загрузка и обработка данных
Вся работа выполняется в Airflow.
Задачи:
1. Полная (инициализирующая) загрузка - DAG initial_load.py.
1.1 Создаём БД (create_database).
1.2 Загружаем данные по API, сохраняем в CSV и PostgreSQL (load_data).
1.3 Создаём промежуточный слой (create_views).
1.4 Формируем итоговую витрину данных (create_mart).
2. Инкрементальная загрузка, выполняется раз в сутки - DAG incremental_load.py.
2.1 Загружаем данные за прошедшие сутки (get\_daily_data).
2.2 Обновляем промежуточный слой (refresh_views).
2.3 Обновляем витрину данных (refresh_mart).

## Хранение данных и построение витрины
Получаемые по API данные сохраняются "как есть" в csv файл (как резерв) и в слой сырых данных в 
Postgres (таблица с именем тикера). При записи в Postgres дублирующиеся значения перезаписываются.

Так как структура данных простая и не содержит общих/дублирующихся полей (за исключением даты/времени), 
решено было не выполнять нормализацию и не сводить данные в одну или несколько таблиц. Вместо этого в 
промежуточном слое для каждого тикера (компании) созданы материализованные представления, в которых предвычислены 
вспомогательные значения для построения итоговой витрины. Остановился на материализованном представлении так как: 
1) для хранения дополнительное место не требуется;
2) быстрый доступ к данным (результаты запроса кэшируются);
3) простота обновления (вручную по команде (реализовано в проекте), либо по триггеру при обновлении исходных таблиц).

Такой же механизм использован и для построения итоговой витрины данных: в cte-запросе строятся временные витрины для 
каждого тикера, которые затем объединяются UNION в одну сводную. SQL скрипты для Postgres генерируются в DAGах 
Airflow по шаблонам, с подстановкой в цикле значений из списка тикеров.

## Нереализованный функционал
Контроль качества данных, сбор статистики обработку ошибок (исключения, сбой загрузки, ошибки сохранения на диск) 
реализовать не успел.

## Улучшения и развитие проекта
Что можно улучшить и добавить:
1. Хранить бэкапы в HDFS - при просте количества данных добавит надёжности и скорости.
2. Использовать Spark - при добавлении витрин и увеличении объёма данных Postgres может перестать справляться. 
Также сложную логику проще реализовать на PySpark/Scala.
3. Разработать web-интерфейс для установки параметров ETL-процесса и выбора тикеров.
4. Добавить визуализацию (дашбоард) метрик качества данных, самого процесса и итоговой витрины.
